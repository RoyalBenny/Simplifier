# -*- coding: utf-8 -*-
"""train_file_12_jun.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qCAgBiQnv1vurr9cpHDt7IztA-CZH_v9
"""

from google.colab import drive
drive.mount('/content/drive')

import re
import numpy as np
import pandas as pd 
from collections import defaultdict
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, Embedding, Dropout, LSTM,Conv1D, GlobalMaxPooling1D
from keras.layers.merge import concatenate
from keras.models import Model
from keras.layers.normalization import BatchNormalization
from keras.callbacks import EarlyStopping, ModelCheckpoint

EMBEDDING_FILE = 'glove.840B.300d.txt'
TRAIN_DATA_FILE = 'train.csv.zip'
TEST_DATA_FILE = 'new_test.csv'

MAX_LEN = 60
MAX_NUM_WORDS = 200000  
EMBEDDING_DIM = 300 
VALIDATION_SPLIT_RATE = 0.1 
HIDDEN_SIZE = 180 
DENSE_SIZE = 128  
DROPOUT_RATE_LSTM = 0.1655 
DROUPOUT_RATE_DENSE = 0.2939  
ACTIVATION_FUNCTION = 'relu'

def text_to_wordlist(sentence):
    
    sentence = sentence.lower().split()
    sentence = " ".join(sentence)

    sentence = re.sub(r"coronavirus", "corona virus 2019", sentence)
    sentence = re.sub(r"covid-19", "corona virus 2019", sentence)
    sentence = re.sub(r"covid - 19", "corona virus 2019", sentence)
    sentence = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", sentence)
    sentence = re.sub(r"what's", "what is ", sentence)
    sentence = re.sub(r"\'s", " ", sentence)
    sentence = re.sub(r"\'ve", " have ", sentence)
    sentence = re.sub(r"can't", "cannot ", sentence)
    sentence = re.sub(r"n't", " not ", sentence)
    sentence = re.sub(r"i'm", "i am ", sentence)
    sentence = re.sub(r"\'re", " are ", sentence)
    sentence = re.sub(r"\'d", " would ", sentence)
    sentence = re.sub(r"\'ll", " will ", sentence)
    sentence = re.sub(r",", " ", sentence)
    sentence = re.sub(r"\.", " ", sentence)
    sentence = re.sub(r"!", " ! ", sentence)
    sentence = re.sub(r"\/", " ", sentence)
    sentence = re.sub(r"\^", " ^ ", sentence)
    sentence = re.sub(r"\+", " + ", sentence)
    sentence = re.sub(r"\-", " - ", sentence)
    sentence = re.sub(r"\=", " = ", sentence)
    sentence = re.sub(r"'", " ", sentence)
    sentence = re.sub(r":", " : ", sentence)
    sentence = re.sub(r"(\d+)(k)", r"\g<1>000", sentence)
    sentence = re.sub(r" e g ", " eg ", sentence)
    sentence = re.sub(r" b g ", " bg ", sentence)
    sentence = re.sub(r" u s ", " american ", sentence)
    sentence = re.sub(r" 9 11 ", "911", sentence)
    sentence = re.sub(r"e - mail", "email", sentence)
    sentence = re.sub(r"j k", "jk", sentence)
    sentence = re.sub(r"\s{2,}", " ", sentence)
    
    return sentence

embeddings_index = {}
f = open(EMBEDDING_FILE, encoding='utf-8')
for line in f:  
    values = line.split()
    word = ''.join(values[:-300])
    embeddings_index[word] = np.asarray(values[-300:], dtype='float32')
f.close()

train_ques_1 = list()  
train_ques_2 = list()  

df_train = pd.read_csv(TRAIN_DATA_FILE, encoding='utf-8')
df_train = df_train.fillna('empty')
train_labels = df_train.is_duplicate.values

for text in df_train.question1.values:
    train_ques_1.append(text_to_wordlist(text))
    
for text in df_train.question2.values:
    train_ques_2.append(text_to_wordlist(text))

print('{} words in train file'.format(len(train_ques_1)))

df_train.head()

test_ques_1 = list()  
test_ques_2 = list()  

df_test = pd.read_csv(TEST_DATA_FILE, encoding='utf-8')  #read only 500 items
df_test = df_test.fillna('empty')
# test_ids = df_test.test_id.values
test_ids = df_test.test_id.values  

for text in df_test.question1.values:
    test_ques_1.append(text_to_wordlist(text))
    
for text in df_test.question2.values:
    test_ques_2.append(text_to_wordlist(text))
    
print('{} words in test file'.format(len(test_ques_1)))

df_test.head()

tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(train_ques_1 + train_ques_2 + test_ques_1 + test_ques_2 )   #fit each words to the tokenizer

#convert each word into corresponding sequence
train_ques_1 = tokenizer.texts_to_sequences(train_ques_1)  
train_ques_2 = tokenizer.texts_to_sequences(train_ques_2)  
test_ques_1 = tokenizer.texts_to_sequences(test_ques_1)  
test_ques_2 = tokenizer.texts_to_sequences(test_ques_2)

word_index = tokenizer.word_index
print('{} unique words/tokens present'.format(len(word_index)))

import pickle

# saving
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

# loading
with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)
word_index = tokenizer.word_index

train_ques_1 = pad_sequences(train_ques_1, maxlen=MAX_LEN)
train_ques_2 = pad_sequences(train_ques_2, maxlen=MAX_LEN)
print('Shape of train data tensor:', train_ques_1.shape)
print('Shape of train labels tensor:', train_labels.shape)

test_ques_1 = pad_sequences(test_ques_1, maxlen=MAX_LEN)
test_ques_2 = pad_sequences(test_ques_2, maxlen=MAX_LEN)
print('Shape of test data vtensor:', test_ques_1.shape)
print('Shape of test ids tensor:', test_ids.shape)

questions = pd.concat([df_train[['question1', 'question2']], df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')
question_dict = defaultdict(set)
for i in range(questions.shape[0]):
        question_dict[questions.question1[i]].add(questions.question2[i])
        question_dict[questions.question2[i]].add(questions.question1[i])

len(question_dict)

# saving
with open('q_dict.pickle', 'wb') as handle:
    pickle.dump(question_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)

df_train['q1_q2_intersect'] = df_train.apply(lambda row : len(set(question_dict[row[3]]).intersection(set(question_dict[row[4]]))) , axis=1, raw=True)
df_train['ques1_freq'] = df_train.apply(lambda row: len(question_dict.get(row[3])) , axis=1, raw=True)
df_train['ques2_freq'] = df_train.apply(lambda row: len(question_dict.get(row[4])), axis=1, raw=True)

df_train.head()

# df_test['q1_q2_intersect'] = df_test.apply(lambda row : len(set(question_dict[row[3]]).intersection(set(question_dict[row[4]]))) , axis=1, raw=True)
# df_test['ques1_freq'] = df_test.apply(lambda row: len(question_dict.get(row[3])) , axis=1, raw=True)
# df_test['ques2_freq'] = df_test.apply(lambda row: len(question_dict.get(row[4])), axis=1, raw=True)

df_test['q1_q2_intersect'] = df_test.apply(lambda row: len(set(question_dict.get(row[1])).intersection(set(question_dict.get(row[2])))), axis=1, raw=True)
df_test['ques1_freq'] = df_test.apply(lambda row: len(question_dict.get(row[1])), axis=1, raw=True)
df_test['ques2_freq'] = df_test.apply(lambda row: len(question_dict.get(row[2])), axis=1, raw=True)

df_test.head()

train_data_features = pd.read_csv('Feature Engineering/Train/feature_nlp+tm.csv')
train_data_features = train_data_features.fillna(0)

test_data_features = pd.read_csv('Feature Engineering/Test/feature_tm+nlp_test.csv')

print('Features(nlp&tm) Loaded')
print(len(df_train))
print(len(df_test))
print(len(train_data_features))
print(len(test_data_features))
#print(len(df_feature_val))

train_data_features.head()

test_data_features.head()

df_train['id'] = train_data_features['id']
df_test['test_id'] = test_data_features['test_id']


df_train = df_train.merge(train_data_features, on='id', how='left')
leak = df_train.drop(['id','qid1','qid2','question1','question2','is_duplicate'], axis=1)
df_test = df_test.merge(test_data_features, on='test_id', how='left')
test_leak = df_test.drop(['test_id','question1','question2'], axis=1)
print('Features created')

leak.head()

test_leak.head()

ss = StandardScaler()
ss.fit(np.vstack((leak, test_leak)))
leaks = ss.transform(leak)  
test_leaks = ss.transform(test_leak) 
print('Normalization finished')

with open('ss.pickle', 'wb') as handle:
    pickle.dump(ss, handle, protocol=pickle.HIGHEST_PROTOCOL)

random_numbers = np.random.permutation(len(train_ques_1))
train_question_indices = random_numbers[:int(len(train_ques_1) * (1 - VALIDATION_SPLIT_RATE))]
test_question_indices = random_numbers[int(len(train_ques_1) * (1 - VALIDATION_SPLIT_RATE)):]
print(train_question_indices)
print(test_question_indices)

data_1_train = np.vstack((train_ques_1[train_question_indices], train_ques_2[train_question_indices]))
data_2_train = np.vstack((train_ques_2[train_question_indices], train_ques_1[train_question_indices]))
leaks_train = np.vstack((leaks[train_question_indices], leaks[train_question_indices]))
labels_train = np.concatenate((train_labels[train_question_indices], train_labels[train_question_indices]))

data_1_val = np.vstack((train_ques_1[test_question_indices], train_ques_2[test_question_indices]))
data_2_val = np.vstack((train_ques_2[test_question_indices], train_ques_1[test_question_indices]))
leaks_val = np.vstack((leaks[test_question_indices], leaks[test_question_indices]))
labels_val = np.concatenate((train_labels[test_question_indices], train_labels[test_question_indices]))

# weight_val = np.ones(len(labels_val))
# weight_val *= 0.5
# weight_val[labels_val==0] = 1.4

from keras.layers import Bidirectional

print('Preparing embedding matrix')

num_words = min(MAX_NUM_WORDS, len(word_index))+1

embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

#creating embedding layer
emb_layer = Embedding(
    input_dim=num_words,
    output_dim=EMBEDDING_DIM,
    weights=[embedding_matrix],
    input_length=MAX_LEN,
    trainable=False
)

#creating bilstm layer
lstm_layer = Bidirectional(LSTM(HIDDEN_SIZE, dropout=DROPOUT_RATE_LSTM, recurrent_dropout=DROPOUT_RATE_LSTM))

#take questions as input
seq1 = Input(shape=(MAX_LEN,), dtype='int32')
seq2 = Input(shape=(MAX_LEN,), dtype='int32')

#attach embedding layer to the model
emb1 = emb_layer(seq1)
emb2 = emb_layer(seq2)

#attach cnn layer to the model
cnn_a = Conv1D(filters=32, kernel_size=8,activation='relu')(emb1)
cnn_b = Conv1D(filters=32, kernel_size=8,activation='relu')(emb2)
cnn_a = GlobalMaxPooling1D()(cnn_a)
cnn_b = GlobalMaxPooling1D()(cnn_b)
cnn_con = concatenate([cnn_a,cnn_b])
cnn_con = BatchNormalization()(cnn_con)
cnn_con = Dropout(DROUPOUT_RATE_DENSE)(cnn_con)
cnn_con = Dense(int(DENSE_SIZE), activation=ACTIVATION_FUNCTION)(cnn_con)

#attach bilstm layer to the model
lstm_a = lstm_layer(emb1)
lstm_b = lstm_layer(emb1)

#take features as input
magic_input = Input(shape=(leaks.shape[1],))
magic_dense = Dense(int(DENSE_SIZE), activation=ACTIVATION_FUNCTION)(magic_input)
lstm_magic_cnn = concatenate([lstm_a,lstm_b,magic_dense])
lstm_magic_cnn = BatchNormalization()(lstm_magic_cnn)
lstm_magic_cnn = Dropout(DROPOUT_RATE_LSTM)(lstm_magic_cnn)
lstm_magic_cnn = Dense(int(DENSE_SIZE), activation=ACTIVATION_FUNCTION)(lstm_magic_cnn)

merged = concatenate([cnn_con,lstm_magic_cnn])
# Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.
merged = BatchNormalization()(merged)
merged = Dropout(DROUPOUT_RATE_DENSE)(merged)

merged = Dense(DENSE_SIZE, activation=ACTIVATION_FUNCTION)(merged)
merged = BatchNormalization()(merged)
merged = Dropout(DROUPOUT_RATE_DENSE)(merged)

preds = Dense(1, activation='sigmoid')(merged)

# class_weight = {0: 1.4, 1: 0.5}

!pip install keras-visualizer

from keras_visualizer import visualizer

model = Model(inputs=[seq1, seq2, magic_input], outputs=preds)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])

model.summary()

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

early_stopping =EarlyStopping(monitor='val_loss', patience=10)
bst_model_path = 'lstm_featured_new_file_2.h5' 
model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)

hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \
        validation_data=([data_1_val, data_2_val, leaks_val], labels_val), \
        epochs=3,  batch_size = 2048, shuffle=True, \
        callbacks=[early_stopping, model_checkpoint])

model.load_weights(bst_model_path) 
bst_val_score = min(hist.history['val_loss'])

model.save('/content/drive/MyDrive/Project/model.h5')

plt.plot(hist.history['acc'])
plt.plot(hist.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train_1', 'test_1'], loc='upper left')
plt.show()

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train_1', 'test_1'], loc='upper left')
plt.show()

# from keras.models import load_model
# model = load_model('/content/drive/MyDrive/Project/new_test_model_7:30pm.h5')

model.evaluate([data_1_val,data_2_val,leaks_val],y = labels_val,verbose=1)

# print('Predict test data')

# preds = model.predict([data_1_val,data_2_val,leaks_val], batch_size=8192, verbose=1,)
# preds += model.predict([data_2_val, data_1_val, leaks_val], batch_size=8192, verbose=1)
# preds /= 2

# for i in range(0,len(preds)):
#   if preds[i]>=0.5:
#     preds[i]=1
#   else:
#     preds[i]=0

# df_train['preds_value'] = preds

# df_train.head()

# from sklearn.metrics import confusion_matrix
# import seaborn as sns
# import matplotlib.pyplot as plt

# cf_matrix = confusion_matrix(df_train['is_duplicate'], df_train['preds_value'])
# print(cf_matrix)

# group_names = ['True Neg','False Pos','False Neg','True Pos']
# group_counts = ["{0:0.0f}".format(value) for value in
#                 cf_matrix.flatten()]
# group_percentages = ["{0:.2%}".format(value) for value in
#                      cf_matrix.flatten()/np.sum(cf_matrix)]
# labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
#           zip(group_names,group_counts,group_percentages)]
# labels = np.asarray(labels).reshape(2,2)
# plt.subplots(figsize=(10,8))
# sns.set(font_scale=2.0)
# sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')

# result = []
# for i,j,k in zip(df_test.question1.values,df_test.question2.values,preds.ravel()):
#   result.append([i,j,k])

# df = pd.DataFrame(data=result,columns=['question1','question2','predict'])

# df.head()

# df.to_csv('predicted.csv')

# msr_test = pd.read_table(r'/content/drive/MyDrive/msr_paraphrase_test.txt',error_bad_lines=False)

# msr_test.head()

# msr_test.to_csv('msr_paraphrase_test.csv')

